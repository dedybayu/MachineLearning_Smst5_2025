{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd644043",
   "metadata": {},
   "source": [
    "| No. Presensi | Nama               | NIM        | Kelas   |\n",
    "| ------------ | ------------------ | ---------- | ------- |\n",
    "| 08           | Dedy Bayu Setiawan | 2341720041 | TI - 3H |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a21a8c0",
   "metadata": {},
   "source": [
    "# Praktikum 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47d1f43",
   "metadata": {},
   "source": [
    "Praktikum 1 ini akan membuat JST sederhana (2 layer) dengan forward pass dan backpropagation manual.&#x20;\n",
    "\n",
    "Backpropagation adalah sebuah algoritma untuk melatih jaringan saraf tiruan dengan cara mengoreksi kesalahan. Algoritma ini bekerja dengan cara menghitung selisih antara keluaran yang dihasilkan jaringan dan keluaran yang seharusnya (kesalahan), lalu memperbarui bobot dan bias jaringan secara berulang dari keluaran ke masukan untuk meminimalkan kesalahan tersebut. Cara kerja backpropagation\n",
    "\n",
    "* **Perambatan maju (*****forward pass*****)**: Data masukan diproses melalui jaringan dari lapisan masukan ke lapisan keluaran untuk menghasilkan prediksi awal.\n",
    "* **Hitung kesalahan**: Selisih antara keluaran prediksi dan keluaran target dihitung menggunakan [fungsi kerugian](https://www.google.com/search?q=fungsi+kerugian\\&oq=backpropagation+adalah\\&gs_lcrp=EgZjaHJvbWUyBggAEEUYOTIGCAEQLhhA0gEINzYwNWowajGoAgiwAgE\\&sourceid=chrome\\&ie=UTF-8\\&ved=2ahUKEwjLoei9vvOQAxVC2TgGHcLNNxUQgK4QegYIAQgAEAo).\n",
    "* **Perambatan mundur (*****backward pass*****)**: Kesalahan disebarkan kembali ke belakang melalui jaringan dari lapisan keluaran ke lapisan masukan untuk menghitung gradien atau turunan parsial dari fungsi kerugian terhadap bobot dan bias.\n",
    "* **Perbarui bobot**: Bobot dan bias disesuaikan menggunakan algoritma penurunan gradien untuk mengurangi kesalahan pada iterasi berikutnya.&#x20;\n",
    "\n",
    "**Langkah:**\n",
    "\n",
    "1. Buat dataset sederhana (XOR).\n",
    "2. Inisialisasi bobot dan bias.\n",
    "3. Implementasikan forward pass.\n",
    "4. Hitung error dan lakukan backpropagation.\n",
    "5. Update bobot menggunakan gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c680493f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.25011021293321906\n",
      "Epoch 1000, Loss: 0.24000648573061084\n",
      "Epoch 2000, Loss: 0.19666312487650955\n",
      "Epoch 3000, Loss: 0.13964613755980235\n",
      "Epoch 4000, Loss: 0.03750209123120704\n",
      "Epoch 5000, Loss: 0.013988794793184715\n",
      "Epoch 6000, Loss: 0.007890599505283091\n",
      "Epoch 7000, Loss: 0.005345773466900567\n",
      "Epoch 8000, Loss: 0.003991916599600825\n",
      "Epoch 9000, Loss: 0.0031635337472450007\n",
      "Prediksi:\n",
      "[[0.0537779 ]\n",
      " [0.95116846]\n",
      " [0.95097706]\n",
      " [0.05251313]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Parameter\n",
    "input_size = 2\n",
    "hidden_size = 2\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Fungsi aktivasi\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hitung error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Output akhir\n",
    "print(\"Prediksi:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45205003",
   "metadata": {},
   "source": [
    "## Tugas 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cadde9a",
   "metadata": {},
   "source": [
    "### Ubah jumlah neuron hidden layer menjadi 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b1d7526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.29084290463253754\n",
      "Epoch 1000, Loss: 0.22379418630162323\n",
      "Epoch 2000, Loss: 0.14911010066213526\n",
      "Epoch 3000, Loss: 0.0514181907439482\n",
      "Epoch 4000, Loss: 0.020105375364507475\n",
      "Epoch 5000, Loss: 0.010708693415872148\n",
      "Epoch 6000, Loss: 0.0068363431324795845\n",
      "Epoch 7000, Loss: 0.004866952078756911\n",
      "Epoch 8000, Loss: 0.0037162413937488047\n",
      "Epoch 9000, Loss: 0.0029764557632569926\n",
      "Prediksi:\n",
      "[[0.03975336]\n",
      " [0.93796207]\n",
      " [0.96617587]\n",
      " [0.05741747]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Parameter\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "# Fungsi aktivasi\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Hitung error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backpropagation\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * sigmoid_derivative(a1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update bobot\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "# Output akhir\n",
    "print(\"Prediksi:\")\n",
    "print(a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d589a0ed",
   "metadata": {},
   "source": [
    "### Bandingkan hasil loss dengan konfigurasi awal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c517484b",
   "metadata": {},
   "source": [
    "**Perbandingan:** Dengan mengubah hiden layer menjadi 3 menghasilkan los akhir lebih kecil (0,00297) dibanding hidden 2 (0.00316), artinya belajar XOR sedikit lebih baik."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b770293",
   "metadata": {},
   "source": [
    "### Tambahkan fungsi aktivasi ReLU dan bandingkan hasil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d20ddd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2746965328487977\n",
      "Epoch 1000, Loss: 0.008823495897671813\n",
      "Epoch 2000, Loss: 0.0021674325089199144\n",
      "Epoch 3000, Loss: 0.001167942341558148\n",
      "Epoch 4000, Loss: 0.0007856027350616149\n",
      "Epoch 5000, Loss: 0.0005872048764353226\n",
      "Epoch 6000, Loss: 0.00046672641447060984\n",
      "Epoch 7000, Loss: 0.0003862020397177809\n",
      "Epoch 8000, Loss: 0.00032871562482117184\n",
      "Epoch 9000, Loss: 0.0002857586213913328\n",
      "Prediksi:\n",
      "[[0.02006726]\n",
      " [0.9898616 ]\n",
      " [0.98991666]\n",
      " [0.02006726]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Dataset XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0],[1],[1],[0]])\n",
    "\n",
    "# Parameter\n",
    "input_size = 2\n",
    "hidden_size = 3\n",
    "output_size = 1\n",
    "lr = 0.1\n",
    "\n",
    "# Inisialisasi bobot\n",
    "W1 = np.random.randn(input_size, hidden_size)\n",
    "b1 = np.zeros((1, hidden_size))\n",
    "W2 = np.random.randn(hidden_size, output_size)\n",
    "b2 = np.zeros((1, output_size))\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10000):\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Error\n",
    "    error = y - a2\n",
    "\n",
    "    # Backprop\n",
    "    d_a2 = error * sigmoid_derivative(a2)\n",
    "    d_W2 = np.dot(a1.T, d_a2)\n",
    "    d_b2 = np.sum(d_a2, axis=0, keepdims=True)\n",
    "\n",
    "    d_a1 = np.dot(d_a2, W2.T) * relu_derivative(z1)\n",
    "    d_W1 = np.dot(X.T, d_a1)\n",
    "    d_b1 = np.sum(d_a1, axis=0, keepdims=True)\n",
    "\n",
    "    # Update\n",
    "    W1 += lr * d_W1\n",
    "    b1 += lr * d_b1\n",
    "    W2 += lr * d_W2\n",
    "    b2 += lr * d_b2\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "print(\"Prediksi:\")\n",
    "print(a2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d113814",
   "metadata": {},
   "source": [
    "**1. Loss**\n",
    "\n",
    "* **Sigmoid:** 0.00297\n",
    "* **ReLU:** **0.00028** (jauh lebih kecil)\n",
    "\n",
    "    **ReLU belajar jauh lebih cepat dan mencapai error yang lebih rendah.**\n",
    "\n",
    "\n",
    "**2. Prediksi**\n",
    "\n",
    "ReLU menghasilkan prediksi yang:\n",
    "\n",
    "* Lebih dekat ke **0** untuk input yang seharusnya 0 (`≈0.02`)\n",
    "* Lebih dekat ke **1** untuk input yang seharusnya 1 (`≈0.99`)\n",
    "\n",
    "    **Prediksi ReLU lebih tajam dan lebih akurat dibanding sigmoid.**\n",
    "\n",
    "\n",
    "**3. Kesimpulan**\n",
    "\n",
    "* ReLU memberikan **konvergensi lebih cepat**, **loss lebih kecil**, dan **prediksi lebih tepat** dibanding hidden sigmoid.\n",
    "* Untuk kasus XOR dengan hidden 3 neuron, **ReLU terbukti performanya paling baik** dari semua konfigurasi sebelumnya."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94440ba5",
   "metadata": {},
   "source": [
    "# Praktikum 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833da272",
   "metadata": {},
   "source": [
    "Padaa praktikum ini kita akan menggunakan library [Keras ](https://keras.io/)untuk menggunakan JST. Keras adalah API tingkat tinggi untuk membangun JST dengan mudah, sedangkan TensorFlow adalah framework yang mendukung Keras.\n",
    "\n",
    "**Langkah:**\n",
    "\n",
    "1. Import library.\n",
    "2. Load dataset.\n",
    "3. Bangun model.\n",
    "4. Kompilasi dan latih model.\n",
    "5. Evaluasi hasil.\n",
    "\n",
    "Klasifikasi Data Iris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12503e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6500 - loss: 0.8294   \n",
      "Epoch 2/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.7164 \n",
      "Epoch 3/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9083 - loss: 0.6531 \n",
      "Epoch 4/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8500 - loss: 0.6115 \n",
      "Epoch 5/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7833 - loss: 0.5790 \n",
      "Epoch 6/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9250 - loss: 0.5493 \n",
      "Epoch 7/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8750 - loss: 0.5231 \n",
      "Epoch 8/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8833 - loss: 0.4980 \n",
      "Epoch 9/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.4783 \n",
      "Epoch 10/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.4606 \n",
      "Epoch 11/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8583 - loss: 0.4519 \n",
      "Epoch 12/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9333 - loss: 0.4388 \n",
      "Epoch 13/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9167 - loss: 0.4186 \n",
      "Epoch 14/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9250 - loss: 0.3988 \n",
      "Epoch 15/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.3887 \n",
      "Epoch 16/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9417 - loss: 0.3832 \n",
      "Epoch 17/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.3610 \n",
      "Epoch 18/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9417 - loss: 0.3618 \n",
      "Epoch 19/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9500 - loss: 0.3411 \n",
      "Epoch 20/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.3367 \n",
      "Epoch 21/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.3228 \n",
      "Epoch 22/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.3195 \n",
      "Epoch 23/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.3082 \n",
      "Epoch 24/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.3024 \n",
      "Epoch 25/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.2915 \n",
      "Epoch 26/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.2819 \n",
      "Epoch 27/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.2727 \n",
      "Epoch 28/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.2674 \n",
      "Epoch 29/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.2594 \n",
      "Epoch 30/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.2533 \n",
      "Epoch 31/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.2490 \n",
      "Epoch 32/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.2382 \n",
      "Epoch 33/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9583 - loss: 0.2347 \n",
      "Epoch 34/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.2257 \n",
      "Epoch 35/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.2215 \n",
      "Epoch 36/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.2187 \n",
      "Epoch 37/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9833 - loss: 0.2096 \n",
      "Epoch 38/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.2046 \n",
      "Epoch 39/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.2008 \n",
      "Epoch 40/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.1980 \n",
      "Epoch 41/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.1900 \n",
      "Epoch 42/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.1886 \n",
      "Epoch 43/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.1875 \n",
      "Epoch 44/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.1790 \n",
      "Epoch 45/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.1820 \n",
      "Epoch 46/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.1837 \n",
      "Epoch 47/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.1694 \n",
      "Epoch 48/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.1730 \n",
      "Epoch 49/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.1690 \n",
      "Epoch 50/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9833 - loss: 0.1617 \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step - accuracy: 0.9667 - loss: 0.1475\n",
      "Akurasi: 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "# One-hot encoding\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Bangun model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(8, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Kompilasi\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Latih model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
    "\n",
    "# Evaluasi\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Akurasi: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8a33de",
   "metadata": {},
   "source": [
    "## Tugas 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eab7e70",
   "metadata": {},
   "source": [
    "### Ubah jumlah neuron hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce57b225",
   "metadata": {},
   "source": [
    "**Hidden: 5 → 3 neuron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa49ab8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3250 - loss: 2.6226  \n",
      "Epoch 2/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3250 - loss: 2.1738 \n",
      "Epoch 3/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3250 - loss: 1.7950 \n",
      "Epoch 4/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.3250 - loss: 1.5390\n",
      "Epoch 5/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3250 - loss: 1.3608 \n",
      "Epoch 6/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3250 - loss: 1.2499 \n",
      "Epoch 7/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2500 - loss: 1.1846 \n",
      "Epoch 8/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.1500 - loss: 1.1416 \n",
      "Epoch 9/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3000 - loss: 1.1149 \n",
      "Epoch 10/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3500 - loss: 1.1019 \n",
      "Epoch 11/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0987 \n",
      "Epoch 12/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3833 - loss: 1.0985 \n",
      "Epoch 13/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3833 - loss: 1.0984 \n",
      "Epoch 14/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0982 \n",
      "Epoch 15/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0981 \n",
      "Epoch 16/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3583 - loss: 1.0981 \n",
      "Epoch 17/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3583 - loss: 1.0980 \n",
      "Epoch 18/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3583 - loss: 1.0979 \n",
      "Epoch 19/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3583 - loss: 1.0978 \n",
      "Epoch 20/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3583 - loss: 1.0977 \n",
      "Epoch 21/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3583 - loss: 1.0977 \n",
      "Epoch 22/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3583 - loss: 1.0977 \n",
      "Epoch 23/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3583 - loss: 1.0976 \n",
      "Epoch 24/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3583 - loss: 1.0975 \n",
      "Epoch 25/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0975 \n",
      "Epoch 26/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3583 - loss: 1.0976 \n",
      "Epoch 27/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0975 \n",
      "Epoch 28/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0974 \n",
      "Epoch 29/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0974 \n",
      "Epoch 30/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0974 \n",
      "Epoch 31/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0973 \n",
      "Epoch 32/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0972 \n",
      "Epoch 33/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0972 \n",
      "Epoch 34/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0972     \n",
      "Epoch 35/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0972 \n",
      "Epoch 36/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0971 \n",
      "Epoch 37/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0971 \n",
      "Epoch 38/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0971 \n",
      "Epoch 39/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0971 \n",
      "Epoch 40/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0970 \n",
      "Epoch 41/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0970 \n",
      "Epoch 42/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0970 \n",
      "Epoch 43/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0970 \n",
      "Epoch 44/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0969 \n",
      "Epoch 45/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0969 \n",
      "Epoch 46/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0969 \n",
      "Epoch 47/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0968 \n",
      "Epoch 48/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3667 - loss: 1.0969 \n",
      "Epoch 49/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0969 \n",
      "Epoch 50/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3667 - loss: 1.0969 \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 127ms/step - accuracy: 0.2333 - loss: 1.1103\n",
      "Akurasi: 0.23333333432674408\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "# One-hot encoding\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Bangun model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(5, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(3, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Kompilasi\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Latih model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
    "\n",
    "# Evaluasi\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Akurasi: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df2c64",
   "metadata": {},
   "source": [
    "**Hidden: 16 → 12 neuron**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3fd5ba93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.3250 - loss: 1.0812   \n",
      "Epoch 2/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.2833 - loss: 1.0242 \n",
      "Epoch 3/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3333 - loss: 0.9771 \n",
      "Epoch 4/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3333 - loss: 0.9155 \n",
      "Epoch 5/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3333 - loss: 0.8603 \n",
      "Epoch 6/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5167 - loss: 0.8172 \n",
      "Epoch 7/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6667 - loss: 0.7807 \n",
      "Epoch 8/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6667 - loss: 0.7512 \n",
      "Epoch 9/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6667 - loss: 0.7230 \n",
      "Epoch 10/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6667 - loss: 0.6935 \n",
      "Epoch 11/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6667 - loss: 0.6671 \n",
      "Epoch 12/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6667 - loss: 0.6395 \n",
      "Epoch 13/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7083 - loss: 0.6105 \n",
      "Epoch 14/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7917 - loss: 0.5837 \n",
      "Epoch 15/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7583 - loss: 0.5609 \n",
      "Epoch 16/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8917 - loss: 0.5322 \n",
      "Epoch 17/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8833 - loss: 0.5096 \n",
      "Epoch 18/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8583 - loss: 0.4855 \n",
      "Epoch 19/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8833 - loss: 0.4671 \n",
      "Epoch 20/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9417 - loss: 0.4504 \n",
      "Epoch 21/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9167 - loss: 0.4288 \n",
      "Epoch 22/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8667 - loss: 0.4165 \n",
      "Epoch 23/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9500 - loss: 0.3965 \n",
      "Epoch 24/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9750 - loss: 0.3876 \n",
      "Epoch 25/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9000 - loss: 0.3739 \n",
      "Epoch 26/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9250 - loss: 0.3581 \n",
      "Epoch 27/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.3514 \n",
      "Epoch 28/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9417 - loss: 0.3374 \n",
      "Epoch 29/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.3331 \n",
      "Epoch 30/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.3173 \n",
      "Epoch 31/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.3105 \n",
      "Epoch 32/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9500 - loss: 0.2995 \n",
      "Epoch 33/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.2912 \n",
      "Epoch 34/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.2830 \n",
      "Epoch 35/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.2752 \n",
      "Epoch 36/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.2701 \n",
      "Epoch 37/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.2602 \n",
      "Epoch 38/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.2567 \n",
      "Epoch 39/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.2473 \n",
      "Epoch 40/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9750 - loss: 0.2429 \n",
      "Epoch 41/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.2340 \n",
      "Epoch 42/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9750 - loss: 0.2297 \n",
      "Epoch 43/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9750 - loss: 0.2234 \n",
      "Epoch 44/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9667 - loss: 0.2208 \n",
      "Epoch 45/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9667 - loss: 0.2143 \n",
      "Epoch 46/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.2078 \n",
      "Epoch 47/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.2028 \n",
      "Epoch 48/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.1976 \n",
      "Epoch 49/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.1923 \n",
      "Epoch 50/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9750 - loss: 0.1874 \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 119ms/step - accuracy: 0.9667 - loss: 0.1683\n",
      "Akurasi: 0.9666666388511658\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "# One-hot encoding\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Bangun model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(16, activation='relu', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(12, activation='relu'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Kompilasi\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Latih model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
    "\n",
    "# Evaluasi\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Akurasi: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4ce785",
   "metadata": {},
   "source": [
    "### Bandingkan akurasi dengan konfigurasi awal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fc7189",
   "metadata": {},
   "source": [
    "**10–8 neuron**\n",
    "\n",
    "Akurasi tinggi (**≈96–98%**) dan loss stabil. Kapasitas jaringan pas sehingga belajar pola dengan baik.\n",
    "Paling seimbang dan performanya bagus.\n",
    "\n",
    "\n",
    "**16–12 neuron**\n",
    "\n",
    "Akurasi juga tinggi (**≈96–99%**) dan loss sedikit lebih rendah. Namun model lebih kompleks dan berpotensi overfitting.\n",
    "Sedikit lebih kuat, tapi tidak jauh lebih baik dari 10–8.\n",
    "\n",
    "\n",
    "**5–3 neuron**\n",
    "Akurasi rendah (**≈35%**) dan loss tidak membaik. Kapasitas terlalu kecil sehingga terjadi underfitting.\n",
    "Performanya paling buruk.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9418c8e3",
   "metadata": {},
   "source": [
    "## Tugas 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a0bc59",
   "metadata": {},
   "source": [
    "### Bandingkan Sigmoid vs ReLU pada dataset Iris."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc8ca1cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.3500 - loss: 1.2660  \n",
      "Epoch 2/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3500 - loss: 1.2159 \n",
      "Epoch 3/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3500 - loss: 1.1823 \n",
      "Epoch 4/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3500 - loss: 1.1512 \n",
      "Epoch 5/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3500 - loss: 1.1333 \n",
      "Epoch 6/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3500 - loss: 1.1179 \n",
      "Epoch 7/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3500 - loss: 1.1084 \n",
      "Epoch 8/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3500 - loss: 1.1001 \n",
      "Epoch 9/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3500 - loss: 1.0942 \n",
      "Epoch 10/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3500 - loss: 1.0889 \n",
      "Epoch 11/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3500 - loss: 1.0851 \n",
      "Epoch 12/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.3500 - loss: 1.0814 \n",
      "Epoch 13/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4667 - loss: 1.0790 \n",
      "Epoch 14/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.4333 - loss: 1.0761 \n",
      "Epoch 15/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6333 - loss: 1.0723     \n",
      "Epoch 16/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 1.0675 \n",
      "Epoch 17/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 1.0625 \n",
      "Epoch 18/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 1.0567 \n",
      "Epoch 19/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 1.0493 \n",
      "Epoch 20/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 1.0412 \n",
      "Epoch 21/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6833 - loss: 1.0299 \n",
      "Epoch 22/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6833 - loss: 1.0204 \n",
      "Epoch 23/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 1.0108 \n",
      "Epoch 24/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 1.0026 \n",
      "Epoch 25/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.9938 \n",
      "Epoch 26/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.9853 \n",
      "Epoch 27/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.9755 \n",
      "Epoch 28/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.9653 \n",
      "Epoch 29/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.9551 \n",
      "Epoch 30/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.9450 \n",
      "Epoch 31/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.9341 \n",
      "Epoch 32/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.9222 \n",
      "Epoch 33/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.9112 \n",
      "Epoch 34/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.8981 \n",
      "Epoch 35/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.8859 \n",
      "Epoch 36/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.8728 \n",
      "Epoch 37/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6833 - loss: 0.8585 \n",
      "Epoch 38/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.8451 \n",
      "Epoch 39/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.8308 \n",
      "Epoch 40/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.8173 \n",
      "Epoch 41/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.8025 \n",
      "Epoch 42/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.7882 \n",
      "Epoch 43/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6833 - loss: 0.7735 \n",
      "Epoch 44/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6917 - loss: 0.7593 \n",
      "Epoch 45/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7000 - loss: 0.7445 \n",
      "Epoch 46/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7000 - loss: 0.7298 \n",
      "Epoch 47/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7000 - loss: 0.7156 \n",
      "Epoch 48/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7000 - loss: 0.7009  \n",
      "Epoch 49/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7250 - loss: 0.6868 \n",
      "Epoch 50/50\n",
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7250 - loss: 0.6730 \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step - accuracy: 0.6000 - loss: 0.6944\n",
      "Akurasi: 0.6000000238418579\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1, 1)\n",
    "\n",
    "# One-hot encoding\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y = encoder.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Bangun model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='sigmoid', input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(8, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Kompilasi\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Latih model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=8)\n",
    "\n",
    "# Evaluasi\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Akurasi: {acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf18b9",
   "metadata": {},
   "source": [
    "### Catat perbedaan loss dan akurasi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769d061",
   "metadata": {},
   "source": [
    "\n",
    "**Hasil Sigmoid**\n",
    "\n",
    "* **Akurasi training naik lambat**, mulai dari 35% dan mentok sekitar **72%**.\n",
    "* **Akurasi testing hanya ~60%** → *buruk*.\n",
    "* **Loss tetap tinggi (0.67–1.26)** dan turun sangat lambat.\n",
    "\n",
    "**Hasil ReLU**\n",
    "\n",
    "* Akurasi jauh lebih tinggi (**>96%**).\n",
    "* Loss cepat menurun hingga **sangat kecil (<0.01)**.\n",
    "* Model stabil dan belajar pola data dengan baik.\n",
    "\n",
    "**Penjelasan Singkat**\n",
    "\n",
    "**Sigmoid** menyebabkan *vanishing gradient*, sehingga pembelajaran lambat, akurasi rendah dan loss tinggi.\n",
    "**ReLU** menjaga gradient tetap besar, training cepat, lebih stabil, dan jauh lebih akurat.\n",
    "\n",
    "**Kesimpulan:** ReLU performanya jauh lebih baik untuk Iris daripada Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60f97f5",
   "metadata": {},
   "source": [
    "# Praktikum 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bac6ba",
   "metadata": {},
   "source": [
    "Praktikum 3 kali ini kita akan mencoba menggunakan Keras untuk Regresi, khususnya pada kasus Prediksi Harga Rumah."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38803228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 587ms/step - loss: 1.0578\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.0452\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.0326\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.0201\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 1.0077\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.9954\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.9832\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.9711\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.9591\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.9472\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.9354\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.9237\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.9120\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.9005\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.8891\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.8778\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.8666\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.8555\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.8445\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.8336\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.8228\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.8122\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.8016\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.7911\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.7808\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.7705\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.7604\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.7504\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.7405\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.7307\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.7210\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.7114\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.7019\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.6925\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.6832\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.6740\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.6650\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.6560\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.6472\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.6384\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.6298\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.6212\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.6128\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.6045\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.5962\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.5881\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.5800\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.5721\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.5643\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5565\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5489\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.5413\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.5338\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5265\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.5192\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.5120\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.5049\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.4979\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.4910\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.4841\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.4774\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.4707\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.4641\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4577\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4512\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4449\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4387\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4325\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4264\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4204\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.4144\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.4086\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.4028\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.3971\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.3914\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3858\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.3803\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.3749\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3695\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.3643\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.3590\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.3539\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.3488\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3437\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.3388\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.3339\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.3291\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.3243\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.3196\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.3150\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.3104\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 0.3059\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.3014\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 0.2970\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2926\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2883\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 0.2841\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - loss: 0.2799\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2757\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 0.2716\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
      "Prediksi: [[-0.30938753]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# Contoh dataset (buat dummy data)\n",
    "data = pd.DataFrame({\n",
    "    'luas': [50, 60, 70, 80, 90],\n",
    "    'harga': [500, 600, 700, 800, 900]\n",
    "})\n",
    "\n",
    "X = data[['luas']]\n",
    "y = data[['harga']]\n",
    "\n",
    "# Normalisasi\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# Evaluasi\n",
    "print(\"Prediksi:\", model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0783d",
   "metadata": {},
   "source": [
    "## Tugas 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6902af2",
   "metadata": {},
   "source": [
    "### Ubah learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee29c10",
   "metadata": {},
   "source": [
    "#### 1. Learning Rate Kecil (lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c1a413c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 540ms/step - loss: 1.1518\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.1404\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 1.1291\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 1.1179\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 1.1067\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.0957\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.0846\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.0737\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.0628\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 1.0520\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.0412\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 1.0306\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.0200\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.0095\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.9990\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.9887\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.9784\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.9682\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - loss: 0.9580\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.9480\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.9380\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.9281\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.9182\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.9085\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.8988\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.8892\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.8796\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.8702\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.8608\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.8515\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.8423\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.8331\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.8240\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.8150\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.8061\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.7972\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.7884\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.7797\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.7710\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.7624\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.7539\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.7455\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.7371\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.7288\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.7206\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.7124\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.7043\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.6962\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.6882\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.6803\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.6725\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.6647\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.6570\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.6493\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - loss: 0.6417\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.6342\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.6267\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.6193\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.6119\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.6046\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.5974\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.5902\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.5830\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.5760\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.5690\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.5620\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.5551\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.5483\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.5415\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.5347\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.5281\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.5214\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.5149\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.5084\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.5019\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.4955\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.4891\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.4828\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - loss: 0.4766\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.4704\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.4643\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.4582\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.4521\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - loss: 0.4461\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.4402\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.4343\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.4285\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.4227\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.4170\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.4113\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.4057\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.4001\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.3946\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.3892\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.3837\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.3784\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.3730\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.3678\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.3626\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.3574\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Prediksi: [[0.31378612]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# Contoh dataset (buat dummy data)\n",
    "data = pd.DataFrame({\n",
    "    'luas': [50, 60, 70, 80, 90],\n",
    "    'harga': [500, 600, 700, 800, 900]\n",
    "})\n",
    "\n",
    "X = data[['luas']]\n",
    "y = data[['harga']]\n",
    "\n",
    "# Normalisasi\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# Evaluasi\n",
    "print(\"Prediksi:\", model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09148319",
   "metadata": {},
   "source": [
    "#### 2. Learning Rate Default (lr = 0.001–0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b042a97f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 656ms/step - loss: 1.4385\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 1.3348\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.2435\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 1.1622\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 1.0881\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 1.0186\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.9539\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.8970\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.8439\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.8001\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - loss: 0.7632\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.7283\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.6954\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.6642\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - loss: 0.6348\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.6069\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.5820\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.5593\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.5378\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.5173\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.4979\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.4796\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.4624\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.4467\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.4363\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.4262\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.4163\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.4067\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.3974\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3883\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.3794\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.3710\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.3627\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.3546\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.3467\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.3389\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.3313\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.3239\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.3167\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - loss: 0.3096\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.3027\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - loss: 0.2959\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.2893\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.2828\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.2765\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.2704\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.2644\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.2585\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.2528\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.2473\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.2418\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.2366\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.2314\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.2264\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.2215\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.2167\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.2120\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.2075\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step - loss: 0.2031\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - loss: 0.1988\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step - loss: 0.1946\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.1905\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.1866\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1827\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.1789\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.1753\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.1717\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.1683\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.1649\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.1617\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.1585\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.1554\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.1524\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.1495\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.1467\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.1440\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.1414\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.1388\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.1363\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.1339\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.1315\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.1292\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.1270\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.1249\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.1228\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.1208\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.1189\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.1170\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.1151\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.1134\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.1116\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.1100\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.1083\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.1068\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.1053\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.1038\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.1024\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step - loss: 0.1010\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0997\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0984\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step\n",
      "Prediksi: [[1.572754]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# Contoh dataset (buat dummy data)\n",
    "data = pd.DataFrame({\n",
    "    'luas': [50, 60, 70, 80, 90],\n",
    "    'harga': [500, 600, 700, 800, 900]\n",
    "})\n",
    "\n",
    "X = data[['luas']]\n",
    "y = data[['harga']]\n",
    "\n",
    "# Normalisasi\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss='mse')\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# Evaluasi\n",
    "print(\"Prediksi:\", model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1285118",
   "metadata": {},
   "source": [
    "#### 3. Learning Rate Besar (lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f35d7dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 411ms/step - loss: 1.2695\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - loss: 0.4890\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - loss: 0.1867\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.2363\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.2254\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.1032\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0274\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0632\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.1298\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.1452\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.1110\n",
      "Epoch 12/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0591\n",
      "Epoch 13/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0197\n",
      "Epoch 14/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0066\n",
      "Epoch 15/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0181\n",
      "Epoch 16/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 0.0401\n",
      "Epoch 17/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 0.0554\n",
      "Epoch 18/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0557\n",
      "Epoch 19/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0446\n",
      "Epoch 20/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0312\n",
      "Epoch 21/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0227\n",
      "Epoch 22/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0204\n",
      "Epoch 23/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0214\n",
      "Epoch 24/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0218\n",
      "Epoch 25/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - loss: 0.0191\n",
      "Epoch 26/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - loss: 0.0136\n",
      "Epoch 27/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0080\n",
      "Epoch 28/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 0.0057\n",
      "Epoch 29/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0083\n",
      "Epoch 30/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0135\n",
      "Epoch 31/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0169\n",
      "Epoch 32/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0155\n",
      "Epoch 33/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0101\n",
      "Epoch 34/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - loss: 0.0047\n",
      "Epoch 35/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0023\n",
      "Epoch 36/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0030\n",
      "Epoch 37/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0052\n",
      "Epoch 38/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0067\n",
      "Epoch 39/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0066\n",
      "Epoch 40/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0054\n",
      "Epoch 41/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0041\n",
      "Epoch 42/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0036\n",
      "Epoch 43/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0039\n",
      "Epoch 44/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 0.0042\n",
      "Epoch 45/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 0.0037\n",
      "Epoch 46/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0024\n",
      "Epoch 47/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - loss: 0.0013\n",
      "Epoch 48/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0011\n",
      "Epoch 49/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 0.0017\n",
      "Epoch 50/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 0.0026\n",
      "Epoch 51/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 0.0029\n",
      "Epoch 52/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 0.0024\n",
      "Epoch 53/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0016\n",
      "Epoch 54/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 0.0010\n",
      "Epoch 55/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 9.7514e-04\n",
      "Epoch 56/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 0.0012\n",
      "Epoch 57/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 0.0012\n",
      "Epoch 58/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 0.0011\n",
      "Epoch 59/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - loss: 8.5201e-04\n",
      "Epoch 60/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 8.0010e-04\n",
      "Epoch 61/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 9.3172e-04\n",
      "Epoch 62/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 0.0011\n",
      "Epoch 63/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 9.7665e-04\n",
      "Epoch 64/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 7.1491e-04\n",
      "Epoch 65/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 4.4943e-04\n",
      "Epoch 66/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.5510e-04\n",
      "Epoch 67/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - loss: 4.3870e-04\n",
      "Epoch 68/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - loss: 5.5441e-04\n",
      "Epoch 69/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 5.6572e-04\n",
      "Epoch 70/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 4.7233e-04\n",
      "Epoch 71/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - loss: 3.7533e-04\n",
      "Epoch 72/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 3.4642e-04\n",
      "Epoch 73/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 3.5950e-04\n",
      "Epoch 74/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 3.4392e-04\n",
      "Epoch 75/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 2.7633e-04\n",
      "Epoch 76/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step - loss: 2.0398e-04\n",
      "Epoch 77/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.8442e-04\n",
      "Epoch 78/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 2.1884e-04\n",
      "Epoch 79/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 2.5335e-04\n",
      "Epoch 80/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 2.4056e-04\n",
      "Epoch 81/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - loss: 1.8729e-04\n",
      "Epoch 82/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 1.3809e-04\n",
      "Epoch 83/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.2230e-04\n",
      "Epoch 84/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 1.2817e-04\n",
      "Epoch 85/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 1.2668e-04\n",
      "Epoch 86/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 1.0849e-04\n",
      "Epoch 87/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - loss: 9.0054e-05\n",
      "Epoch 88/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - loss: 8.7538e-05\n",
      "Epoch 89/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 49ms/step - loss: 9.4976e-05\n",
      "Epoch 90/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - loss: 9.3059e-05\n",
      "Epoch 91/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 7.4370e-05\n",
      "Epoch 92/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 5.1864e-05\n",
      "Epoch 93/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 4.1918e-05\n",
      "Epoch 94/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 4.5548e-05\n",
      "Epoch 95/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 5.0190e-05\n",
      "Epoch 96/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - loss: 4.6401e-05\n",
      "Epoch 97/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 3.7348e-05\n",
      "Epoch 98/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - loss: 3.1541e-05\n",
      "Epoch 99/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 3.0905e-05\n",
      "Epoch 100/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - loss: 2.9721e-05\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "Prediksi: [[-1.2605073]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "# Contoh dataset (buat dummy data)\n",
    "data = pd.DataFrame({\n",
    "    'luas': [50, 60, 70, 80, 90],\n",
    "    'harga': [500, 600, 700, 800, 900]\n",
    "})\n",
    "\n",
    "X = data[['luas']]\n",
    "y = data[['harga']]\n",
    "\n",
    "# Normalisasi\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "y = scaler.fit_transform(y)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, activation='relu', input_shape=(1,)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.1), loss='mse')\n",
    "model.fit(X_train, y_train, epochs=100)\n",
    "\n",
    "# Evaluasi\n",
    "print(\"Prediksi:\", model.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8fe8d7",
   "metadata": {},
   "source": [
    "### Bandingkan hasil loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a83ddd",
   "metadata": {},
   "source": [
    "**1. Learning Rate Kecil (0.001)**\n",
    "\n",
    "* Loss akhir: **0.3574** (masih besar)\n",
    "* Penurunan loss **lambat**\n",
    "* Prediksi kurang akurat (**0.31**)\n",
    "    \n",
    "    **Sangat lambat, belum mendekati nilai sebenarnya.**\n",
    "\n",
    "**2. Learning Rate Default (0.01)**\n",
    "\n",
    "* Loss akhir: **0.0984** (lebih baik)\n",
    "* Loss turun lebih cepat\n",
    "* Prediksi lebih masuk akal (**1.57**)\n",
    "\n",
    "    **Lebih stabil dan lebih cepat mencapai loss kecil.**\n",
    "\n",
    "**3. Learning Rate Besar (0.1)**\n",
    "\n",
    "* Loss akhir: **0.000029** (sangat kecil)\n",
    "* Loss turun **sangat cepat**\n",
    "* Namun prediksi menjadi **aneh (-1.26)**\n",
    "\n",
    "    **Secara loss terlihat bagus, tetapi model menjadi tidak stabil.**\n",
    "\n",
    "**Kesimpulan:**\n",
    "\n",
    "* **LR kecil →** stabil tapi lambat, loss masih tinggi.\n",
    "* **LR sedang →** hasil paling baik dan stabil.\n",
    "* **LR besar →** loss kecil tapi prediksi rusak karena model *overshoot* dan tidak stabil."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad37cce4",
   "metadata": {},
   "source": [
    "## Praktikum berikut akan menggunakan data Boston untuk memprediksi harga rumah.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72782997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 845us/step\n",
      "RMSE: 0.510755327068471\n"
     ]
    }
   ],
   "source": [
    "# MLP regresi (Keras)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 1. Load\n",
    "data = fetch_california_housing()\n",
    "X = data.data; y = data.target\n",
    "\n",
    "# 2. Preprocess\n",
    "scaler = StandardScaler(); Xs = scaler.fit_transform(X)\n",
    "X_train, X_val, y_train, y_val = train_test_split(Xs, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Build model\n",
    "model = Sequential([\n",
    "Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "Dense(32, activation='relu'),\n",
    "Dense(1)\n",
    "])\n",
    "model.compile(optimizer=Adam(1e-3), loss='mse', metrics=['mae'])\n",
    "\n",
    "# 4. Train\n",
    "h = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=200, batch_size=32, verbose=0)\n",
    "\n",
    "# 5. Plot\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1); plt.plot(h.history['loss'], label='train_loss'); plt.plot(h.history['val_loss'], label='val_loss'); plt.legend(); plt.title('MSE')\n",
    "plt.subplot(1,2,2); plt.plot(h.history['mae'], label='train_mae'); plt.plot(h.history['val_mae'], label='val_mae'); plt.legend(); plt.title('MAE')\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "pred = model.predict(X_val)\n",
    "print('RMSE:', np.sqrt(mean_squared_error(y_val, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c33b161",
   "metadata": {},
   "source": [
    "# Tugas Praktikum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2aa81b",
   "metadata": {},
   "source": [
    "Gunakan JST untuk klasifikasi angka tulisan tangan (MNIST).\n",
    "\n",
    "**Langkah:**\n",
    "\n",
    "* Load dataset MNIST dari Keras.\n",
    "* Bangun model dengan 2 hidden layer.\n",
    "* Latih model dan evaluasi akurasi.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6008c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9299 - loss: 0.2363\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9693 - loss: 0.0997\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9781 - loss: 0.0709\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9834 - loss: 0.0530\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9863 - loss: 0.0407\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9767 - loss: 0.0764\n",
      "Akurasi pada data uji: 0.9767\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time\n",
    "\n",
    "# 1. Load dataset MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalisasi\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# One-hot label\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 2. Bangun Model (2 hidden layer)\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),         # ubah 28x28 menjadi vektor 784\n",
    "    Dense(128, activation='relu'),         # hidden layer 1\n",
    "    Dense(64, activation='relu'),          # hidden layer 2\n",
    "    Dense(10, activation='softmax')        # output layer\n",
    "])\n",
    "\n",
    "# 3. Kompilasi\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4. Latih model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# 5. Evaluasi model\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Akurasi pada data uji: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c280d9",
   "metadata": {},
   "source": [
    "Coba dengan beberapa parameter lain:\n",
    "\n",
    "* Ubah jumlah neuron di hidden layer (misal: 256 dan 128).\n",
    "* Tambahkan satu hidden layer lagi.\n",
    "* Bandingkan akurasi dan waktu pelatihan.\n",
    "* Eksperimen dengan fungsi aktivasi **Sigmoid** vs **ReLU**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df837c1",
   "metadata": {},
   "source": [
    "### Ubah jumlah neuron hidden layer → 256 & 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b56bdcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9380 - loss: 0.2083\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9733 - loss: 0.0847\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9814 - loss: 0.0575\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9858 - loss: 0.0429\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9887 - loss: 0.0339\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9737 - loss: 0.0889\n",
      "Akurasi pada data uji: 0.9737\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time\n",
    "\n",
    "# 1. Load dataset MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalisasi\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# One-hot label\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 2. Bangun Model (2 hidden layer)\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28,28)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# 3. Kompilasi\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4. Latih model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# 5. Evaluasi model\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Akurasi pada data uji: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eee88ae",
   "metadata": {},
   "source": [
    "### Tambahkan hidden layer ke-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae528fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.9354 - loss: 0.2143\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9721 - loss: 0.0905\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9800 - loss: 0.0637\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9841 - loss: 0.0499\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9872 - loss: 0.0403\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9803 - loss: 0.0639\n",
      "Akurasi pada data uji: 0.9803\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time\n",
    "\n",
    "# 1. Load dataset MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalisasi\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# One-hot label\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 2. Bangun Model (2 hidden layer)\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28,28)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),  # tambahan\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# 3. Kompilasi\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4. Latih model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# 5. Evaluasi model\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Akurasi pada data uji: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefb5c39",
   "metadata": {},
   "source": [
    "### Perbandingan ReLU vs Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57a4cbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.8928 - loss: 0.3855\n",
      "Epoch 2/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9543 - loss: 0.1538\n",
      "Epoch 3/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9702 - loss: 0.1011\n",
      "Epoch 4/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9776 - loss: 0.0733\n",
      "Epoch 5/5\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9829 - loss: 0.0551\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9744 - loss: 0.0804\n",
      "Akurasi pada data uji: 0.9744\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import time\n",
    "\n",
    "# 1. Load dataset MNIST\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Normalisasi\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# One-hot label\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# 2. Bangun Model (2 hidden layer)\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28,28)),\n",
    "    Dense(256, activation='sigmoid'),\n",
    "    Dense(128, activation='sigmoid'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "\n",
    "# 3. Kompilasi\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 4. Latih model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=32)\n",
    "\n",
    "# 5. Evaluasi model\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Akurasi pada data uji: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b9322b",
   "metadata": {},
   "source": [
    "**Penjelasan**\n",
    "\n",
    "1. **Model awal (128 & 64, 2 Hidden Layer)**\n",
    "   Akurasi uji **0.9767** → sudah sangat baik, stabil, dan cepat dilatih.\n",
    "\n",
    "2. **Jumlah neuron diperbesar (256 & 128)**\n",
    "   Akurasi uji **sedikit turun menjadi 0.9737** meskipun akurasi training naik.\n",
    "   Artinya model **mulai overfitting**: lebih kompleks, tapi tidak meningkatkan performa uji.\n",
    "\n",
    "3. **Menambah hidden layer ke-3**\n",
    "   Akurasi uji naik menjadi **0.9803** (tertinggi).\n",
    "   Layer tambahan memberi kemampuan belajar fitur yang lebih dalam tanpa overfitting yang besar.\n",
    "\n",
    "4. **Mengganti aktivasi menjadi Sigmoid**\n",
    "   Akurasi uji **0.9744**, lebih rendah dibanding ReLU.\n",
    "   Sigmoid cenderung lebih lambat dan dapat mengalami vanishing gradient.\n",
    "\n",
    "\n",
    "**Kesimpulan**\n",
    "\n",
    "* **Model terbaik** adalah **3 hidden layer** (akurasi uji **0.9803**).\n",
    "* **Menambah neuron saja** tidak selalu meningkatkan performa; malah bisa overfitting.\n",
    "* **ReLU lebih unggul** dibanding Sigmoid untuk MNIST karena lebih cepat dan stabil.\n",
    "* Secara keseluruhan, **kombinasi arsitektur yang seimbang (bukan terlalu besar) + ReLU + layer ekstra** memberikan hasil optimal.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
