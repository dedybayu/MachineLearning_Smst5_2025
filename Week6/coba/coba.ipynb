{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c92614d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 1. Import Library\n",
    "# =========================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da43d0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (692703, 79)\n"
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 2. Load Dataset\n",
    "# =========================================================\n",
    "df = pd.read_csv(\"data/Wednesday-workingHours.pcap_ISCX.csv\")\n",
    "print(\"Data shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d522649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 3. Pisahkan fitur dan label\n",
    "# =========================================================\n",
    "kolom_drop = [\n",
    "    \"Flow ID\", \"Source IP\", \"Destination IP\",\n",
    "    \"Source Port\", \"Destination Port\", \"Timestamp\"\n",
    "]\n",
    "\n",
    "for kol in kolom_drop:\n",
    "    if kol in df.columns:\n",
    "        df.drop(columns=kol, inplace=True)\n",
    "\n",
    "# Simpan label asli\n",
    "if \"Label\" in df.columns:\n",
    "    labels_true = df[\"Label\"].copy()\n",
    "    df.drop(columns=[\"Label\"], inplace=True)\n",
    "else:\n",
    "    labels_true = None\n",
    "\n",
    "# Hapus baris kosong\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f74d565",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m X \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[np\u001b[38;5;241m.\u001b[39mnumber])\n\u001b[0;32m      5\u001b[0m scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m----> 6\u001b[0m X_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mfit_transform(X)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# 5. Sampling maksimum 10.000 data setelah normalisasi\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# =========================================================\u001b[39;00m\n\u001b[0;32m     11\u001b[0m sample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;241m10000\u001b[39m, \u001b[38;5;28mlen\u001b[39m(X_scaled))\n",
      "File \u001b[1;32mc:\\Users\\dedyb\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:319\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 319\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    322\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    323\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    324\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    325\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\dedyb\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:918\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    903\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    904\u001b[0m             (\n\u001b[0;32m    905\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    913\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m    914\u001b[0m         )\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    920\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\dedyb\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:894\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 894\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_fit(X, y, sample_weight)\n",
      "File \u001b[1;32mc:\\Users\\dedyb\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dedyb\\anaconda3\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:930\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    898\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    899\u001b[0m \n\u001b[0;32m    900\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    927\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    929\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 930\u001b[0m X \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    932\u001b[0m     X,\n\u001b[0;32m    933\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    934\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    935\u001b[0m     ensure_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    936\u001b[0m     reset\u001b[38;5;241m=\u001b[39mfirst_call,\n\u001b[0;32m    937\u001b[0m )\n\u001b[0;32m    938\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\dedyb\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:2944\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2942\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2943\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m-> 2944\u001b[0m     out \u001b[38;5;241m=\u001b[39m check_array(X, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m   2945\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m   2946\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\dedyb\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[1;32m-> 1107\u001b[0m     _assert_all_finite(\n\u001b[0;32m   1108\u001b[0m         array,\n\u001b[0;32m   1109\u001b[0m         input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m   1110\u001b[0m         estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m   1111\u001b[0m         allow_nan\u001b[38;5;241m=\u001b[39mensure_all_finite \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1112\u001b[0m     )\n\u001b[0;32m   1114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1116\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dedyb\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 120\u001b[0m _assert_all_finite_element_wise(\n\u001b[0;32m    121\u001b[0m     X,\n\u001b[0;32m    122\u001b[0m     xp\u001b[38;5;241m=\u001b[39mxp,\n\u001b[0;32m    123\u001b[0m     allow_nan\u001b[38;5;241m=\u001b[39mallow_nan,\n\u001b[0;32m    124\u001b[0m     msg_dtype\u001b[38;5;241m=\u001b[39mmsg_dtype,\n\u001b[0;32m    125\u001b[0m     estimator_name\u001b[38;5;241m=\u001b[39mestimator_name,\n\u001b[0;32m    126\u001b[0m     input_name\u001b[38;5;241m=\u001b[39minput_name,\n\u001b[0;32m    127\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\dedyb\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "# =========================================================\n",
    "# 4. Normalisasi SELURUH DATA (baru nanti di-sampling)\n",
    "# =========================================================\n",
    "X = df.select_dtypes(include=[np.number])\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# =========================================================\n",
    "# 5. Sampling maksimum 10.000 data setelah normalisasi\n",
    "# =========================================================\n",
    "sample_size = min(10000, len(X_scaled))\n",
    "sample_indices = np.random.choice(len(X_scaled), sample_size, replace=False)\n",
    "\n",
    "X_sample = X_scaled[sample_indices]\n",
    "if labels_true is not None:\n",
    "    y_sample = labels_true.iloc[sample_indices].reset_index(drop=True)\n",
    "else:\n",
    "    y_sample = None\n",
    "\n",
    "print(f\"Jumlah data diambil: {len(X_sample)} dari total {len(X_scaled)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee7e856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 6. Reduksi dimensi untuk visualisasi\n",
    "# =========================================================\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "X_pca = pca.fit_transform(X_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85325aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 7. KMeans Clustering\n",
    "# =========================================================\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "labels_kmeans = kmeans.fit_predict(X_sample)\n",
    "\n",
    "sil_k = silhouette_score(X_sample, labels_kmeans)\n",
    "db_k = davies_bouldin_score(X_sample, labels_kmeans)\n",
    "\n",
    "print(\"\\n=== Evaluasi KMeans ===\")\n",
    "print(f\"Silhouette Score : {sil_k:.4f}\")\n",
    "print(f\"Davies-Bouldin Index : {db_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98fb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 8. DBSCAN Clustering\n",
    "# =========================================================\n",
    "dbscan = DBSCAN(eps=1.5, min_samples=10)\n",
    "labels_dbscan = dbscan.fit_predict(X_sample)\n",
    "\n",
    "unique_db = np.unique(labels_dbscan)\n",
    "if len(unique_db) > 1:\n",
    "    sil_db = silhouette_score(X_sample, labels_dbscan)\n",
    "    db_db = davies_bouldin_score(X_sample, labels_dbscan)\n",
    "else:\n",
    "    sil_db, db_db = None, None\n",
    "\n",
    "print(\"\\n=== Evaluasi DBSCAN ===\")\n",
    "print(f\"Cluster unik : {unique_db}\")\n",
    "if sil_db is not None:\n",
    "    print(f\"Silhouette Score : {sil_db:.4f}\")\n",
    "    print(f\"Davies-Bouldin Index : {db_db:.4f}\")\n",
    "else:\n",
    "    print(\"DBSCAN menghasilkan 1 cluster (tidak bisa dihitung silhouette).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60c1486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 9. Visualisasi hasil clustering\n",
    "# =========================================================\n",
    "def plot_cluster(X_pca, labels, title):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.scatterplot(x=X_pca[:,0], y=X_pca[:,1], hue=labels, palette=\"tab10\", s=15)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "plot_cluster(X_pca, labels_kmeans, \"KMeans Clustering (PCA 2D)\")\n",
    "plot_cluster(X_pca, labels_dbscan, \"DBSCAN Clustering (PCA 2D)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 10. Analisis perbandingan terhadap label asli\n",
    "# =========================================================\n",
    "if y_sample is not None:\n",
    "    result = pd.DataFrame({\n",
    "        \"Label_True\": y_sample.values,\n",
    "        \"KMeans\": labels_kmeans,\n",
    "        \"DBSCAN\": labels_dbscan\n",
    "    })\n",
    "\n",
    "    print(\"\\n=== Distribusi Cluster KMeans vs Label Asli ===\")\n",
    "    print(result.groupby(\"KMeans\")[\"Label_True\"].value_counts(), \"\\n\")\n",
    "\n",
    "    print(\"=== Distribusi Cluster DBSCAN vs Label Asli ===\")\n",
    "    print(result.groupby(\"DBSCAN\")[\"Label_True\"].value_counts(), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f62a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 11. Analisis fitur pembeda antar cluster (KMeans)\n",
    "# =========================================================\n",
    "df_features = pd.DataFrame(X_sample, columns=X.columns)\n",
    "df_features[\"Cluster_KMeans\"] = labels_kmeans\n",
    "\n",
    "cluster_means = df_features.groupby(\"Cluster_KMeans\").mean()\n",
    "diff = cluster_means.T\n",
    "if 0 in diff.columns and 1 in diff.columns:\n",
    "    diff[\"Difference\"] = abs(diff[0] - diff[1])\n",
    "else:\n",
    "    diff[\"Difference\"] = diff.max(axis=1) - diff.min(axis=1)\n",
    "\n",
    "print(\"=== Fitur yang Paling Membedakan antar Cluster (KMeans) ===\")\n",
    "print(diff.sort_values(\"Difference\", ascending=False).head(10))\n",
    "\n",
    "# =========================================================\n",
    "# 12. Analisis Noise DBSCAN\n",
    "# =========================================================\n",
    "noise_ratio = np.sum(labels_dbscan == -1) / len(labels_dbscan)\n",
    "print(f\"\\n=== Analisis Noise DBSCAN ===\")\n",
    "print(f\"Jumlah data noise: {(labels_dbscan == -1).sum()} dari {len(labels_dbscan)} \"\n",
    "      f\"({noise_ratio*100:.2f}%)\")\n",
    "\n",
    "if y_sample is not None:\n",
    "    print(result[result[\"DBSCAN\"] == -1][\"Label_True\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3f27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# 13. Kesimpulan Otomatis\n",
    "# =========================================================\n",
    "print(\"\\n================== KESIMPULAN ==================\")\n",
    "\n",
    "# a. Apakah cluster memisahkan normal vs serangan?\n",
    "if y_sample is not None:\n",
    "    distribusi = result.groupby(\"KMeans\")[\"Label_True\"].value_counts(normalize=True).unstack().fillna(0)\n",
    "    print(\"\\n>> Apakah cluster memisahkan normal vs serangan?\")\n",
    "    print(distribusi)\n",
    "    if (distribusi.max().max() > 0.8):\n",
    "        print(\"✅ Cluster KMeans cukup baik memisahkan normal dan serangan.\")\n",
    "    else:\n",
    "        print(\"⚠️  Cluster KMeans belum memisahkan dengan baik.\")\n",
    "\n",
    "# b. Apakah DBSCAN menemukan noise menarik?\n",
    "print(\"\\n>> Apakah DBSCAN menemukan noise menarik?\")\n",
    "if noise_ratio > 0.05:\n",
    "    print(f\"✅ Ya, sekitar {noise_ratio*100:.2f}% data terdeteksi sebagai noise (kemungkinan anomali).\")\n",
    "else:\n",
    "    print(f\"⚠️  Tidak banyak noise yang terdeteksi (hanya {noise_ratio*100:.2f}%).\")\n",
    "\n",
    "# c. Fitur pembeda\n",
    "print(\"\\n>> Fitur utama yang membedakan cluster:\")\n",
    "print(diff.sort_values(\"Difference\", ascending=False).head(5))\n",
    "\n",
    "# d. Apakah clustering membantu eksplorasi?\n",
    "print(\"\\n>> Apakah clustering cukup membantu?\")\n",
    "if sil_k > 0.3 or (sil_db is not None and sil_db > 0.3):\n",
    "    print(\"✅ Ya, clustering membantu memahami pola trafik (indikasi cluster cukup terpisah).\")\n",
    "else:\n",
    "    print(\"⚠️  Clustering belum terlalu efektif untuk memisahkan pola trafik secara jelas.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
